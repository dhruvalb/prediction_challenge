{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Build a Predictor\n",
    "## Dhruval Bhatt (Collaborated with Keertana Chidambaram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions from assignment:\n",
    "\n",
    "Build a predictor . Once you have built a predictor, you should use the test data set to submit a\n",
    "csv file on canvas. That csv file should only have two columns: diag_id, y_hat.\n",
    "\n",
    "y_hat = drinks per day = 'U1031900'\n",
    "diag_id = 'diag.id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Set (7187, 4887)\n",
      "Size of Test Set (1797, 4886)\n",
      "Size of Training Set after Removing Col 1 (7187, 4886)\n",
      "Size of Test Set after Removing Col 1 (1797, 4885)\n"
     ]
    }
   ],
   "source": [
    "# Load the data given\n",
    "\n",
    "train_df = pd.read_csv('../Data/nlsy training set.csv')\n",
    "test_df = pd.read_csv('../Data/nlsy test set.csv')\n",
    "\n",
    "print(\"Size of Training Set\", train_df.shape)\n",
    "print(\"Size of Test Set\", test_df.shape)\n",
    "\n",
    "train_df.drop(train_df.columns[[0]], axis=1, inplace=True)  #remove unnamed column that is the repeat of id column in train\n",
    "test_df.drop(test_df.columns[[0]], axis=1, inplace=True)#remove unnamed column that is the repeat of id column in test\n",
    "\n",
    "print(\"Size of Training Set after Removing Col 1\", train_df.shape)\n",
    "print(\"Size of Test Set after Removing Col 1\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.set_index('diag.id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows that have missing values for the target data\n",
    "train_df = train_df[train_df['U1031900'] >= 0] #subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Set X variables (3545, 4884)\n"
     ]
    }
   ],
   "source": [
    "train_X = train_df.loc[:, train_df.columns != 'U1031900']\n",
    "print(\"Size of Training Set X variables\", train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Set y variables (3545,)\n"
     ]
    }
   ],
   "source": [
    "train_y = train_df['U1031900']\n",
    "print(\"Size of Training Set y variables\", train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2836, 4884)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variance of y_all: 15.280341115678567\n",
      "The variance of y_train: 14.793511864340045\n",
      "The variance of y_test: 17.250743069334526\n"
     ]
    }
   ],
   "source": [
    "print(\"The variance of y_all:\", train_df['U1031900'].var())\n",
    "print(\"The variance of y_train:\", y_train.var())\n",
    "print(\"The variance of y_test:\", y_test.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection with Lasso (Applied to RF and XBG model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruval\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False),\n",
       "        norm_order=1, prefit=False, threshold=0.25)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, LogisticRegression, LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.fillna(0))\n",
    "\n",
    "clf = Lasso(alpha=0.001)\n",
    "\n",
    "#sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1'))\n",
    "sel_ = SelectFromModel(clf, threshold=0.25)\n",
    "sel_.fit(scaler.transform(X_train.fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 4884\n",
      "selected features: 1112\n",
      "features with coefficients shrank to zero: 1955\n"
     ]
    }
   ],
   "source": [
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "      np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2836, 1112), (709, 1112))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sel_.estimator_.coef_ == 0)\n",
    "selected_feat\n",
    "\n",
    "X_train_selected = sel_.transform(X_train.fillna(0))\n",
    "X_test_selected = sel_.transform(X_test.fillna(0))\n",
    "\n",
    "X_train_selected.shape, X_test_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['E5270800'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with RFE\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# feature extraction\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X_train, y_train)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelimnary Models: Decision Tree, no tuning and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruval\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "model_1 = tree.DecisionTreeClassifier()\n",
    "model_1.fit(X_train, y_train)\n",
    "pred_1 = model_1.predict(X_test)\n",
    "mse_1 = mean_squared_error(y_test, pred_1)\n",
    "print('MSE: %f' % mse_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 27.550071\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "model_3 = tree.DecisionTreeClassifier()\n",
    "model_3.fit(X_train_selected, y_train)\n",
    "pred_3 = model_3.predict(X_test_selected)\n",
    "mse_3 = mean_squared_error(y_test, pred_3)\n",
    "print('MSE: %f' % mse_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model_1.feature_importances_, index= X_train.columns)\n",
    "#feat_importances.nlargest(10).plot(kind='barh')\n",
    "#plt.show()\n",
    "\n",
    "selected_features_1 = list(feat_importances.nlargest(2000).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with Selected (Just kept the top 2000 predictors)\n",
    "\n",
    "X_train_sel = X_train.filter(selected_features_1)\n",
    "X_test_sel = X_test.filter(selected_features_1)\n",
    "\n",
    "model_2 = tree.DecisionTreeClassifier()\n",
    "model_2.fit(X_train_sel, y_train)\n",
    "pred_2 = model_2.predict(X_test_sel)\n",
    "mse_2 = mean_squared_error(y_test, pred_2)\n",
    "print('MSE: %f' % mse_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Meta Methods Applied - RF, XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Decision Tree\n",
    "\n",
    "rf_c = RandomForestClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=10, \n",
    "                                 n_estimators=25, max_features=4000, random_state=0)\n",
    "rf_c.fit(X_train, y_train)\n",
    "pred_rfc = rf_c.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, pred_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB Regressor\n",
    "# xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "#                 max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "xgb_r = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 15, random_state=0)\n",
    "xgb_r.fit(X_train,y_train)\n",
    "pred_xgbr = xgb_r.predict(X_test)\n",
    "pred_xgbr = numpy.around(pred_xgbr)\n",
    "mean_squared_error(y_test, pred_xgbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection with Lasso (Applied to RF and XBG model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.64174894217207"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RFC\n",
    "fs_rf_c = RandomForestClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=10, \n",
    "                                 n_estimators=25, random_state=0)\n",
    "fs_rf_c.fit(X_train_selected, y_train)\n",
    "fs_pred_rfc = fs_rf_c.predict(X_test_selected)\n",
    "\n",
    "mean_squared_error(y_test, fs_pred_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruval\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.88293370944993"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGB Regressor - Selected \n",
    "# xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "#                 max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "fs_xgb_r = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 15, random_state=0)\n",
    "fs_xgb_r.fit(X_train_selected,y_train)\n",
    "\n",
    "fs_pred_xgbr = fs_xgb_r.predict(X_test_selected)\n",
    "fs_pred_xgbr = numpy.around(fs_pred_xgbr)\n",
    "\n",
    "mean_squared_error(y_test, fs_pred_xgbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Final Model to Given Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Final Model to Test Data: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To DO: \n",
    "# Add - RF Regressor \n",
    "# Neural Network\n",
    "# Linear Regression \n",
    "# SVM\n",
    "# Ensemble \n",
    "#Final Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
